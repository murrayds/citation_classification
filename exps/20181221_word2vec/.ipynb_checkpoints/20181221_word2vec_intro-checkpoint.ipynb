{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>pub_month</th>\n",
       "      <th>n_authors</th>\n",
       "      <th>n_references</th>\n",
       "      <th>n_sections</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>n_citation_sentence</th>\n",
       "      <th>reference_seq</th>\n",
       "      <th>section_seq</th>\n",
       "      <th>paragraph_seq</th>\n",
       "      <th>sentence_seq</th>\n",
       "      <th>citation_characer_seq</th>\n",
       "      <th>n_citations_in_sentence</th>\n",
       "      <th>sentence_character_seq</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10.1016/j.joi.2006.05.001</td>\n",
       "      <td>The influence of missing publications on the H...</td>\n",
       "      <td>Bar-Ilan, 2006; Egghe, in press; Gl채nzel, 2006...</td>\n",
       "      <td>Recently the Hirsch index, in short: h-index, ...</td>\n",
       "      <td>2007</td>\n",
       "      <td>24085</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>74</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Bar-Ilan, 2006; Egghe, in press; Gl채nzel, 2006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10.1016/j.joi.2006.05.001</td>\n",
       "      <td>The influence of missing publications on the H...</td>\n",
       "      <td>Hirsch (2005)</td>\n",
       "      <td>This index, introduced by &lt;CITATION&gt; is calcul...</td>\n",
       "      <td>2007</td>\n",
       "      <td>24085</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>74</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>Hirsch (2005)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10.1016/j.joi.2006.05.001</td>\n",
       "      <td>The influence of missing publications on the H...</td>\n",
       "      <td>Braun et al., 2005</td>\n",
       "      <td>Clearly, this definition can also be applied t...</td>\n",
       "      <td>2007</td>\n",
       "      <td>24085</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>74</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>653</td>\n",
       "      <td>3</td>\n",
       "      <td>525</td>\n",
       "      <td>Braun et al., 2005||Egghe &amp; Rousseau, 2006||Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10.1016/j.joi.2006.05.001</td>\n",
       "      <td>The influence of missing publications on the H...</td>\n",
       "      <td>Bar-Ilan, 2006</td>\n",
       "      <td>Yet, it is also possible to collect citations ...</td>\n",
       "      <td>2007</td>\n",
       "      <td>24085</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>74</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>973</td>\n",
       "      <td>1</td>\n",
       "      <td>894</td>\n",
       "      <td>Bar-Ilan, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10.1016/j.joi.2006.05.001</td>\n",
       "      <td>The influence of missing publications on the H...</td>\n",
       "      <td>Rousseau, 2005</td>\n",
       "      <td>Expressed in a conglomerate framework this mea...</td>\n",
       "      <td>2007</td>\n",
       "      <td>24085</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>74</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1160</td>\n",
       "      <td>1</td>\n",
       "      <td>1078</td>\n",
       "      <td>Rousseau, 2005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                        doi  \\\n",
       "0           1  10.1016/j.joi.2006.05.001   \n",
       "1           2  10.1016/j.joi.2006.05.001   \n",
       "2           3  10.1016/j.joi.2006.05.001   \n",
       "3           4  10.1016/j.joi.2006.05.001   \n",
       "4           5  10.1016/j.joi.2006.05.001   \n",
       "\n",
       "                                               title  \\\n",
       "0  The influence of missing publications on the H...   \n",
       "1  The influence of missing publications on the H...   \n",
       "2  The influence of missing publications on the H...   \n",
       "3  The influence of missing publications on the H...   \n",
       "4  The influence of missing publications on the H...   \n",
       "\n",
       "                                               label  \\\n",
       "0  Bar-Ilan, 2006; Egghe, in press; Gl채nzel, 2006...   \n",
       "1                                      Hirsch (2005)   \n",
       "2                                 Braun et al., 2005   \n",
       "3                                     Bar-Ilan, 2006   \n",
       "4                                     Rousseau, 2005   \n",
       "\n",
       "                                                text  pub_year  pub_month  \\\n",
       "0  Recently the Hirsch index, in short: h-index, ...      2007      24085   \n",
       "1  This index, introduced by <CITATION> is calcul...      2007      24085   \n",
       "2  Clearly, this definition can also be applied t...      2007      24085   \n",
       "3  Yet, it is also possible to collect citations ...      2007      24085   \n",
       "4  Expressed in a conglomerate framework this mea...      2007      24085   \n",
       "\n",
       "   n_authors  n_references  n_sections  n_sentences  n_citation_sentence  \\\n",
       "0          1            13           7           74                   22   \n",
       "1          1            13           7           74                   22   \n",
       "2          1            13           7           74                   22   \n",
       "3          1            13           7           74                   22   \n",
       "4          1            13           7           74                   22   \n",
       "\n",
       "   reference_seq  section_seq  paragraph_seq  sentence_seq  \\\n",
       "0              1            1              1             1   \n",
       "1             10            1              1             2   \n",
       "2              2            1              1             5   \n",
       "3              1            1              2             7   \n",
       "4             12            1              2             8   \n",
       "\n",
       "   citation_characer_seq  n_citations_in_sentence  sentence_character_seq  \\\n",
       "0                    109                        4                       1   \n",
       "1                    196                        1                     170   \n",
       "2                    653                        3                     525   \n",
       "3                    973                        1                     894   \n",
       "4                   1160                        1                    1078   \n",
       "\n",
       "                                              labels  \n",
       "0  Bar-Ilan, 2006; Egghe, in press; Gl채nzel, 2006...  \n",
       "1                                      Hirsch (2005)  \n",
       "2  Braun et al., 2005||Egghe & Rousseau, 2006||Ro...  \n",
       "3                                     Bar-Ilan, 2006  \n",
       "4                                     Rousseau, 2005  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('https://www.dropbox.com/s/sgyhxd859iw4xx0/prepared_sentences.csv?dl=1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets just make sure that we can get the word2vec model to work. Here, I am using the gensim implementation of word2vec, and I am following the example provided on their website at the following link,\n",
    "\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# Load some basic text to use with word2vec. Common_texts has a simple text set that is useful for \n",
    "# testing. \n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "# Load the actual function\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "print(common_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.1489480e-03  4.6315109e-03 -1.2491714e-03  3.1914932e-03\n",
      " -3.9946549e-03  2.8592069e-03 -2.4089201e-03  1.6210233e-03\n",
      "  3.8524044e-03  5.7751627e-04 -2.2796197e-03 -3.9831321e-03\n",
      "  4.5976015e-03 -2.1081872e-03  3.3956424e-03  2.5681325e-05\n",
      " -3.7307497e-03 -4.2543071e-03 -4.4249659e-03  4.2128242e-03\n",
      "  4.5555532e-03  8.0007710e-04 -3.9744959e-03  4.9724225e-03\n",
      "  2.5508124e-03 -4.0595321e-04  2.6521916e-03 -1.5699866e-03\n",
      " -1.7158827e-03 -3.3637073e-03 -2.9873680e-03  2.0031317e-04\n",
      "  3.8014823e-03  4.9986877e-03 -4.2100502e-03 -4.0316009e-03\n",
      " -1.0168720e-03  2.9728594e-03  1.6256475e-03  3.1714817e-03\n",
      " -2.5440832e-03 -3.4327798e-03 -3.9537209e-03  2.5396782e-03\n",
      "  1.6764089e-03 -4.2403364e-03  7.6707022e-04 -4.9221292e-03\n",
      " -4.4140830e-03  2.8244916e-03  1.3735527e-03  3.0899441e-03\n",
      "  4.1663033e-04  4.9184300e-03 -1.7166643e-03  8.9128356e-04\n",
      "  3.8822315e-04 -8.4111042e-04 -3.4197553e-03  1.3200253e-03\n",
      "  1.1575234e-03 -3.3262321e-03  2.4013519e-03 -3.2004828e-03\n",
      "  4.1664005e-03  2.9250223e-04  4.9132789e-03 -1.4426276e-03\n",
      " -3.7093959e-03 -2.9191687e-03  9.5326849e-04 -4.3647117e-03\n",
      " -3.8374521e-04 -2.5150522e-03  1.4337520e-03  1.5941465e-04\n",
      " -2.2663400e-03 -2.6615018e-03 -8.5237285e-04  4.1206449e-04\n",
      "  2.6562584e-03  4.0167035e-03 -1.8195986e-03  3.4610631e-03\n",
      " -2.3560176e-04 -1.4669932e-03  2.3631586e-03  2.2728029e-03\n",
      "  1.0029672e-03  2.3311949e-03 -3.6672838e-03 -2.2776469e-03\n",
      " -3.7763196e-03  2.8847842e-04  1.3229579e-03 -4.6403403e-03\n",
      "  9.8834932e-04 -4.4776923e-03  1.3185747e-03  4.0138252e-03]\n"
     ]
    }
   ],
   "source": [
    "# Using the gensim functions, load the path to the tmpfile\n",
    "path = get_tmpfile(\"word2vec.model\")\n",
    "\n",
    "# Create the word2vec model, using the common texts library\n",
    "model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Train the model with \n",
    "model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)\n",
    "(0, 2)\n",
    "\n",
    "vector = model.wv['computer']  # numpy vector of a word\n",
    "\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we know that something is working, I'll move on to completing the tutorial from the following url,\n",
    "\n",
    "http://ai.intelligentonlinetools.com/ml/k-means-clustering-example-word2vec/\n",
    "\n",
    "The author defines their own sentences for use, which I will copy directly for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [['this', 'is', 'the', 'good', 'machine', 'learning', 'book'],\n",
    "            ['this', 'is',  'another', 'book'],\n",
    "            ['one', 'more', 'book'],\n",
    "            ['this', 'is', 'the', 'new', 'post'],\n",
    "                        ['this', 'is', 'about', 'machine', 'learning', 'post'],  \n",
    "            ['and', 'this', 'is', 'the', 'last', 'post']]\n",
    "\n",
    "model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the example, we can query the dictionary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'the', 'good', 'machine', 'learning', 'book', 'another', 'one', 'more', 'new', 'post', 'about', 'and', 'last']\n"
     ]
    }
   ],
   "source": [
    "print(list(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also query the model similarity between any two words, as we do below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.085407786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dakotamurray/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('this', 'book'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will repeat this process, but with a sample of my own data. I will need to do some basic pre-processing on the sentences in my dataframe and tokenize the words. \n",
    "\n",
    "First, lets make sure that we can process a single line,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This index, introduced by <CITATION> is calculated as follows.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'index',\n",
       " 'introduced',\n",
       " 'by',\n",
       " 'citation',\n",
       " 'is',\n",
       " 'calculated',\n",
       " 'as',\n",
       " 'follows']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.utils as gs_utils\n",
    "line = data[\"text\"][1]\n",
    "print(line)\n",
    "gs_utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20407\n",
      "['this', 'index', 'introduced', 'by', 'citation', 'is', 'calculated', 'as', 'follows']\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for line in data[\"text\"]:\n",
    "    sentences.append(gs_utils.simple_preprocess(line))\n",
    "    \n",
    "print(len(sentences))\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a list of lists. Each index of this list corresponds to a sentence, and each sentence is a tokenized list of word. Now, we can proceed to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15179\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sentences, min_count=1)\n",
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if we wanted to exmaine similar words form this corpus, all we have to do is query as we do below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dakotamurray/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('uk', 0.970356285572052),\n",
       " ('canada', 0.9672210812568665),\n",
       " ('netherlands', 0.9637432098388672),\n",
       " ('united', 0.9604684710502625),\n",
       " ('astrophysics', 0.9600337743759155),\n",
       " ('literatures', 0.959436297416687),\n",
       " ('eu', 0.9584054350852966),\n",
       " ('programs', 0.9578442573547363),\n",
       " ('schools', 0.9576901197433472),\n",
       " ('korea', 0.9572107791900635)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('usa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The data here is fairly small (around 20,000 sentences), and while it seems to perform well for \"common\" words, it doesn't produce as meaningful results for less common words. If we want to make more meaningful representations, we need more data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
